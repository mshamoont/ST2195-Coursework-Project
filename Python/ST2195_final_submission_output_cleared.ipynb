{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ef02d58",
   "metadata": {},
   "source": [
    "# ST2195 Coursework Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c81199e",
   "metadata": {},
   "source": [
    "# PART-1\n",
    "\n",
    "In this part, We are asked to work with the Markov Chain Monte Carlo algorithm, in particular the Metropolis-Hastings algorithm. The aim is to simulate random numbers for the distribution with probability density function given below  <br>\n",
    "f(x) = 0.5 * exp(−|x|), <br>\n",
    "where x takes values in the real line and |x| denotes the absolute value of x. More specifically, you are asked to generate x0, x1, . . . , xN values and store them using the following version of the Metropolis-Hastings algorithm (also known as random walk Metropolis) that consists of the steps below:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4489d793",
   "metadata": {},
   "source": [
    "### Setting up Random Walk Metropolis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ea0767",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb990b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probability density function f(x)\n",
    "# Target distribution f(x) = 1/2 * exp(-|x|)\n",
    "def f(x):\n",
    "    return 0.5 * np.exp(-np.abs(x))  \n",
    "\n",
    "# Setting up initial values\n",
    "x0 = 0  # Initial value of x\n",
    "N = 10000  # Number of iterations\n",
    "s = 1.0  # Standard deviation (step size) for the proposal distribution\n",
    "\n",
    "# Array to store the generated values\n",
    "x_values = np.zeros(N + 1)\n",
    "x_values[0] = x0  \n",
    "\n",
    "# Random walk Metropolis-Hastings\n",
    "for i in range(1, N + 1):\n",
    "    \n",
    "    x_star = np.random.normal(x_values[i - 1], s)\n",
    "    r = f(x_star) / f(x_values[i - 1])\n",
    "    \n",
    "    # u from Uniform(0, 1)\n",
    "    u = np.random.uniform(0, 1)\n",
    "    \n",
    "    # Acceptance or Rejection criteria based on the ratio\n",
    "    if u < r:\n",
    "        x_values[i] = x_star  # Accept the new value\n",
    "    else:\n",
    "        x_values[i] = x_values[i - 1]  # Reject the new value, stay at the current value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2e8488",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca54258",
   "metadata": {},
   "source": [
    "## Answer for Question 1.a\n",
    "<br>\n",
    "Apply the random walk Metropolis algorithm using N = 10000 and s = 1. Use the generated samples (x1, . . . xN ) to construct a histogram and a kernel density plot in the same figure. Note that these provide estimates of f (x).Overlay a graph of f (x) on this figure to visualise the quality of these estimates. Also, report the sample mean and standard deviation of the generated samples (Note: these are also known as the Monte Carlo estimates of the mean and standard deviation respectively).\n",
    "Practical tip: To avoid numerical errors, it is better to use the equivalent criterion log u < log r (x∗, xi−1) = log f (x∗) − log f (xi−1) instead of u < r (x∗, xi−1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0042a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random walk Metropolis-Hastings\n",
    "for i in range(1, N + 1):\n",
    "    \n",
    "    x_star = np.random.normal(x_values[i - 1], s)\n",
    "    \n",
    "    # Ratio using log r(x*, x[i-1])\n",
    "    log_r = np.log(f(x_star)) - np.log(f(x_values[i - 1]))   \n",
    "    \n",
    "    # u from Uniform(0, 1)\n",
    "    u = np.random.uniform(0, 1)\n",
    "    \n",
    "    # # Acceptance or Rejection criteria based on the new ratio\n",
    "    if np.log(u) < log_r:\n",
    "        x_values[i] = x_star  # Accept the new value\n",
    "    else:\n",
    "        x_values[i] = x_values[i - 1]  # Reject the new value\n",
    "\n",
    "# Sample mean and standard deviation\n",
    "sample_mean = np.mean(x_values)\n",
    "sample_std = np.std(x_values)\n",
    "\n",
    "\n",
    "print(f\"Sample Mean: {sample_mean}\")\n",
    "print(f\"Sample Standard Deviation: {sample_std}\")\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Histogram and kernel density estimate\n",
    "sns.histplot(x_values[1:], bins=50, stat=\"density\", color='g', label='Histogram', kde=True)\n",
    "\n",
    "# Overlay the target distribution\n",
    "x = np.linspace(-10, 10, 1000)\n",
    "plt.plot(x, f(x), 'r-', label='Target Distribution f(x)')\n",
    "\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Metropolis-Hastings Sampling')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e1da28",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae51693",
   "metadata": {},
   "source": [
    "## Answer for Question 1.b\n",
    "<br>\n",
    "The operations in part 1(a) are based on the assumption that the algorithm has converged. One of the most widely used convergence diagnostics is the so-called Rb value. In order to obtain a valued of this diagnostic, you need to apply the procedure below:\n",
    "• Generate more than one sequence of x0,...,xN, potentially using different\n",
    "initial values x0. Denote each of these sequences, also known as chains, by\n",
    "(x(j),x(j),...,x(j)) for j = 1,2,...,J. 01N\n",
    "• Define and compute Mj as the sample mean of chain j as 1N\n",
    "i=1\n",
    "and Vj as the within sample variance of chain j as\n",
    "Mj = Xx(j). Ni\n",
    "1N\n",
    "Vj = X(x(j) −Mj)2.\n",
    "Ni i=1\n",
    "• Define and compute the overall within sample variance W as 1 XJ\n",
    "W=J Vj j=1\n",
    "• Define and compute the overall sample mean M as 1 XJ\n",
    "M=J Mj, j=1\n",
    "and the between sample variance B as 1 XJ\n",
    "• Compute the Rb value as\n",
    "In general, values of Rb close to 1 indicate convergence, and it is usually desired for Rb to be lower than 1.05. Calculate the Rb for the random walk Metropolis algorithm with N = 2000, s = 0.001 and J = 4. Keeping N and J fixed, provide a plot of the values of Rb over a grid of s values in the interval between 0.001 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22575720",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Target distribution function f(x)\n",
    "def f(x):\n",
    "    return 0.5 * np.exp(-np.abs(x))\n",
    "\n",
    "# Metropolis-Hastings algorithm for a single chain\n",
    "def metropolis_hastings_chain(x0, N, s, burn_in=500):\n",
    "    x_values = np.zeros(N + burn_in)  \n",
    "    x_values[0] = x0\n",
    "    for i in range(1, N + burn_in):\n",
    "        x_star = np.random.normal(x_values[i-1], s)\n",
    "        log_r = np.log(f(x_star)) - np.log(f(x_values[i-1]))\n",
    "        u = np.random.uniform(0, 1)\n",
    "        if np.log(u) < log_r:\n",
    "            x_values[i] = x_star\n",
    "        else:\n",
    "            x_values[i] = x_values[i-1]\n",
    "    \n",
    "    # Chain after the burn-in period\n",
    "    return x_values[burn_in:]\n",
    "\n",
    "# R_hat value\n",
    "def compute_r_hat(chains):\n",
    "    J = len(chains)  \n",
    "    N = len(chains[0])  \n",
    "    \n",
    "    #  Mj(mean of each chain) and Vj(variance of each chain)\n",
    "    Mj = np.mean(chains, axis=1)\n",
    "    Vj = np.var(chains, axis=1, ddof=1)\n",
    "    \n",
    "    # Overall mean M\n",
    "    M = np.mean(Mj)\n",
    "    \n",
    "    # Within-sample variance W\n",
    "    W = np.mean(Vj)\n",
    "    \n",
    "    # Between-sample variance B\n",
    "    B = np.mean((Mj - M)**2)\n",
    "    \n",
    "    # R_hat\n",
    "    R_hat = np.sqrt((B + W) / W)\n",
    "    return R_hat\n",
    "\n",
    "# Multiple chains and R_hat over a grid of s values\n",
    "N = 2000  \n",
    "J = 4  \n",
    "burn_in = 500  \n",
    "initial_values = [0, 1, -1, 2]  \n",
    "s_values = np.linspace(0.01, 1, 100)  \n",
    "r_hat_values = []\n",
    "\n",
    "# Cap for R_hat values\n",
    "r_hat_cap = 2  \n",
    "\n",
    "for s in s_values:\n",
    "    chains = []\n",
    "    for x0 in initial_values:\n",
    "        chain = metropolis_hastings_chain(x0, N, s, burn_in=burn_in)\n",
    "        chains.append(chain)\n",
    "    chains = np.array(chains)\n",
    "    r_hat = compute_r_hat(chains)\n",
    "    \n",
    "    # Capped R_hat value for visualization\n",
    "    if r_hat > r_hat_cap:\n",
    "        r_hat = r_hat_cap  # Cap the R_hat value\n",
    "    r_hat_values.append(r_hat)\n",
    "\n",
    "# Plotting\n",
    "plt.plot(s_values, r_hat_values, label=r'$\\hat{R}$ (capped)')\n",
    "plt.axhline(y=1.05, color='r', linestyle='--', label='Threshold 1.05')\n",
    "plt.xlabel('Step size (s)')\n",
    "plt.ylabel(r'$\\hat{R}$ value')\n",
    "plt.title(r'$\\hat{R}$ values for different step sizes s (capped)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5de6a82",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2c7e19",
   "metadata": {},
   "source": [
    "# PART-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441fc27d",
   "metadata": {},
   "source": [
    "#### The 2009 ASA Statistical Computing and Graphics Data Expo consisted of flight arrival and departure details for all commercial flights on major carriers within the USA from Oc- tober 1987 to April 2008. This is a large dataset; there are nearly 120 million records in total, and it takes up 1.6 gigabytes of space when compressed and 12 gigabytes when un- compressed. The complete dataset, along with supplementary information and variable descriptions, can be downloaded from the Harvard Dataverse at https://doi.org/10.7910/DVN/HG7NV7\n",
    "Choose any subset of ten consecutive years and any of the supplementary information provided by the Harvard Dataverse to answer the following questions using the principles and tools you have learned in this course:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674c322a",
   "metadata": {},
   "source": [
    "## I am choosing 10 years period of 1996-2005 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d7bde1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Uploading Downloaded flights data to SQL\n",
    "\n",
    "import os\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "csv_folder_path = \"/Users/muhammadshamoontariq/Desktop/LSE_Prog practice assignments/dataverse_files/Analysis_data_csv\"  # Your folder path with CSV files\n",
    "db_file_path = \"flights_data.db\"  # Path to your SQLite database file\n",
    "\n",
    "# Connecting to SQL\n",
    "conn = sqlite3.connect(db_file_path)\n",
    "\n",
    "# Looping through 14 files\n",
    "for file in os.listdir(csv_folder_path):\n",
    "    if file.endswith(\".csv\"):  \n",
    "        file_path = os.path.join(csv_folder_path, file)\n",
    "        table_name = os.path.splitext(file)[0]\n",
    "\n",
    "        print(f\"Processing file: {file}\")  \n",
    "\n",
    "        try:\n",
    "            # Encountered encoding error so add exception\n",
    "            df = pd.read_csv(file_path, encoding='utf-8', on_bad_lines='skip')\n",
    "        except UnicodeDecodeError:\n",
    "            df = pd.read_csv(file_path, encoding='ISO-8859-1', on_bad_lines='skip')\n",
    "\n",
    "        # Uploading the DataFrame to SQLite\n",
    "        df.to_sql(table_name, conn, if_exists='replace', index=False)\n",
    "        print(f\"Uploaded {file} to the '{table_name}' table.\")\n",
    "\n",
    "\n",
    "conn.close()\n",
    "print(\"All CSV files have been uploaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69d7d74",
   "metadata": {},
   "source": [
    "## Answer to Question 2 a)\n",
    "\n",
    "### What are the best times and days of the week to minimise delays each year?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45244426",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87661a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connecting to the SQLite database\n",
    "conn = sqlite3.connect('flights_data.db')  # Update with your actual database name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce81186",
   "metadata": {},
   "source": [
    "#### First anlyzing only one year (2005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0441b4d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#inspecting data\n",
    "year = '2005'\n",
    "query = f'SELECT * FROM \"{year}\";'\n",
    "flights_2005 = pd.read_sql_query(query, conn)\n",
    "flights_2005.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2207abe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetching data only for Departure time, Day of the week & Departure Delays\n",
    "query_1 = f'SELECT DepTime, DayOfWeek, DepDelay FROM \"{year}\";'\n",
    "df = pd.read_sql_query(query_1, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddc3e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing\n",
    "# Converting object data type to Numeric AND removing NA values\n",
    "df['DepTime'] = pd.to_numeric(df['DepTime'], errors='coerce')\n",
    "df['DepDelay'] = pd.to_numeric(df['DepDelay'], errors='coerce')\n",
    "df.dropna(inplace=True)  # Remove NA values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928c8a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting DepTime into time categories (e.g., morning, afternoon, evening)\n",
    "# Adding a new column 'TimeCategory' based on DepTime ranges\n",
    "def categorize_time(time):\n",
    "    if pd.isnull(time):\n",
    "        return 'Unknown'\n",
    "    time = int(time)\n",
    "    if 0 <= time < 600:\n",
    "        return 'Late Night'\n",
    "    elif 600 <= time < 1200:\n",
    "        return 'Morning'\n",
    "    elif 1200 <= time < 1800:\n",
    "        return 'Afternoon'\n",
    "    else:\n",
    "        return 'Evening'\n",
    "\n",
    "df['TimeCategory'] = df['DepTime'].apply(categorize_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb69a46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by DayOfWeek and TimeCategory, and calculate the average departure delay\n",
    "delay_summary = df.groupby(['DayOfWeek', 'TimeCategory'])['DepDelay'].mean().reset_index()\n",
    "\n",
    "\n",
    "# Pivot table to create a heatmap-friendly format\n",
    "pivot_df = delay_summary.pivot(index='DayOfWeek', columns='TimeCategory', values='DepDelay')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6804533f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(pivot_df, cmap='coolwarm', annot=True, fmt='.2f')\n",
    "plt.title('Average Departure Delay by Day of Week and Time of Day')\n",
    "plt.ylabel('Day of Week')\n",
    "plt.xlabel('Time of Day')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e92aa4",
   "metadata": {},
   "source": [
    "# 10-year period (1996 to 2005) Analysis\n",
    "\n",
    "### Best times and days of the week to minimise delays each year?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c64ea0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Setting up range of the years\n",
    "years = [str(year) for year in range(1996, 2006)]  \n",
    "\n",
    "# dictionary to store results for each year\n",
    "delay_summaries = {}\n",
    "\n",
    "for year in years:\n",
    "    \n",
    "    query = f'SELECT DepTime, DayOfWeek, DepDelay FROM \"{year}\";'\n",
    "    df = pd.read_sql_query(query, conn)\n",
    "\n",
    "   \n",
    "    df['DepTime'] = pd.to_numeric(df['DepTime'], errors='coerce')\n",
    "    df['DepDelay'] = pd.to_numeric(df['DepDelay'], errors='coerce')\n",
    "    df.dropna(inplace=True)  # Remove NA values\n",
    "\n",
    "\n",
    "    # Converting DepTime into time categories (e.g., morning, afternoon, evening)\n",
    "    def categorize_time(time):\n",
    "        if pd.isnull(time):\n",
    "            return 'Unknown'\n",
    "        time = int(time)\n",
    "        if 0 <= time < 600:\n",
    "            return 'Late Night'\n",
    "        elif 600 <= time < 1200:\n",
    "            return 'Morning'\n",
    "        elif 1200 <= time < 1800:\n",
    "            return 'Afternoon'\n",
    "        else:\n",
    "            return 'Evening'\n",
    "\n",
    "    df['TimeCategory'] = df['DepTime'].apply(categorize_time)\n",
    "\n",
    "    # Group by DayOfWeek and TimeCategory, and calculate the average departure delay\n",
    "    delay_summary = df.groupby(['DayOfWeek', 'TimeCategory'])['DepDelay'].mean().reset_index()\n",
    "\n",
    "    # Adding the 'Year' column to the delay summary df\n",
    "    delay_summary['Year'] = year\n",
    "\n",
    "    delay_summaries[year] = delay_summary\n",
    "\n",
    "    # Pivot table to create a heatmap-friendly format\n",
    "    pivot_df = delay_summary.pivot(index='DayOfWeek', columns='TimeCategory', values='DepDelay')\n",
    "\n",
    "    # Heatmap\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.heatmap(pivot_df, cmap='coolwarm', annot=True, fmt='.2f')\n",
    "    plt.title(f'Average Departure Delay by Day of Week and Time of Day for {year}')\n",
    "    plt.ylabel('Day of Week (1=Mon, 7=Sun)')\n",
    "    plt.xlabel('Time of Day')\n",
    "    plt.show()\n",
    "\n",
    "# Combining all the delay summaries into a single DataFrame for further analysis\n",
    "combined_delay_summary = pd.concat(delay_summaries.values(), ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f15b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_delay_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65d3182",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# average delays for each year\n",
    "avg_delays_per_year = {year: delay_summary['DepDelay'].mean() for year, delay_summary in delay_summaries.items()}\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(avg_delays_per_year.keys(), avg_delays_per_year.values(), color='skyblue')\n",
    "plt.title('Average Departure Delays by Year')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Average Delay (minutes)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "avg_delays_per_year_df = pd.DataFrame(list(avg_delays_per_year.items()), columns=['Year', 'Average_DepDelay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be09e6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "avg_delays_per_year_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac72da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouped bar chart\n",
    "grouped_delay_summary = {}\n",
    "for year, summary in delay_summaries.items():\n",
    "    for time_cat in summary['TimeCategory'].unique():\n",
    "        avg_delay = summary[summary['TimeCategory'] == time_cat]['DepDelay'].mean()\n",
    "        grouped_delay_summary.setdefault(time_cat, {})[year] = avg_delay\n",
    "\n",
    "# plotting\n",
    "grouped_delay_df = pd.DataFrame(grouped_delay_summary).fillna(0)\n",
    "\n",
    "grouped_delay_df.plot(kind='bar', figsize=(12, 6))\n",
    "plt.title('Average Departure Delays by Time Category and Year')\n",
    "plt.xlabel('Time Category')\n",
    "plt.ylabel('Average Delay (minutes)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Year')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c2b8c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grouped_delay_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d84ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimum delays by DayOfWeek and TimeCategory for each year\n",
    "optimal_times = {}\n",
    "\n",
    "for year, summary in delay_summaries.items():\n",
    "    min_delay = summary.loc[summary['DepDelay'].idxmin()]\n",
    "    optimal_times[year] = {\n",
    "        'Best Day': min_delay['DayOfWeek'],\n",
    "        'Best Time Category': min_delay['TimeCategory'],\n",
    "        'Min Delay': min_delay['DepDelay']\n",
    "    }\n",
    "\n",
    "optimal_times_df = pd.DataFrame(optimal_times).T\n",
    "print(\"Optimal Times and Days for Each Year:\")\n",
    "optimal_times_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6312e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum delay by DayOfWeek and TimeCategory for each year\n",
    "max_delay_times = {}\n",
    "\n",
    "for year, summary in delay_summaries.items():\n",
    "    max_delay = summary.loc[summary['DepDelay'].idxmax()]  # Find the row with the max average delay\n",
    "    max_delay_times[year] = {\n",
    "        'Worst Day': max_delay['DayOfWeek'],\n",
    "        'Worst Time Category': max_delay['TimeCategory'],\n",
    "        'Max Delay': max_delay['DepDelay']\n",
    "    }\n",
    "\n",
    "max_delay_df = pd.DataFrame(max_delay_times).T\n",
    "print(\"Maximum Delays for Each Year:\")\n",
    "max_delay_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb488c4",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686d6773",
   "metadata": {},
   "source": [
    "# Answer to Question 2 b)\n",
    "\n",
    "### Evaluate whether older planes suffer more delays on a year-to-year basis?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7f75e1",
   "metadata": {},
   "source": [
    "#### First Analyzing only one year (2005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1c5cd9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "year_of_analysis = 2005  \n",
    "\n",
    "# Getting Arrival delays and Tail-number from flights data table and joining on Tail-Number with Planes table to get Year of Manufacture  \n",
    "query = '''\n",
    "SELECT f.ArrDelay, f.TailNum, p.Year as ManufactureYear\n",
    "FROM \"2005\" f  \n",
    "LEFT JOIN \"plane-data\" p ON f.TailNum = p.TailNum\n",
    "WHERE f.Year = ?;  \n",
    "'''\n",
    "\n",
    "# Execute the query with the year_of_analysis parameter\n",
    "df_age = pd.read_sql_query(query, conn, params=(year_of_analysis,))\n",
    "df_age.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff5cedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Data Preprocessing\n",
    "#Converting Object type to Numeric\n",
    "df_age['ArrDelay'] = pd.to_numeric(df_age['ArrDelay'], errors='coerce')  \n",
    "df_age['ManufactureYear'] = pd.to_numeric(df_age['ManufactureYear'], errors='coerce')  \n",
    "\n",
    "# Removing NA rows (Also include 0 values as some fields are missing data)\n",
    "df_age.dropna(subset=['ArrDelay', 'ManufactureYear'], inplace=True)  \n",
    "\n",
    "#Counting invalid values (Removing planes which were provided same year or after the year under anlysis)\n",
    "\n",
    "same_year_count = (df_age['ManufactureYear'] == year_of_analysis).sum()\n",
    "beyond_year_count = (df_age['ManufactureYear'] > year_of_analysis).sum()\n",
    "\n",
    "# Total count of invalid values (not strictly necessary for removal)\n",
    "total_invalid_count = same_year_count + beyond_year_count\n",
    "\n",
    "\n",
    "# indices of rows to drop\n",
    "indices_to_drop = df_age[\n",
    "    (df_age['ManufactureYear'] == year_of_analysis) |  # Same year\n",
    "    (df_age['ManufactureYear'] > year_of_analysis) |   # Beyond year of analysis\n",
    "    (df_age['ManufactureYear'] == 0)                    # Zero values (Some data is missing in the provided database)\n",
    "].index\n",
    "\n",
    "df_age.drop(indices_to_drop, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcec173e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Counting how many values in 'year' are null\n",
    "null_count = df_age['ManufactureYear'].isnull().sum()\n",
    "\n",
    "# Counting how many values in 'year' are empty strings\n",
    "empty_count = (df_age['ManufactureYear'] == \"\").sum()\n",
    "\n",
    "# Counting how many values in 'year' are zeero\n",
    "zero_count = (df_age['ManufactureYear'] == 0).sum()\n",
    "\n",
    "\n",
    "# Counting how many values in 'year' are 2005\n",
    "same_year_count = (df_age['ManufactureYear'] == 2005).sum()\n",
    "\n",
    "# Counting how many values in 'year' are greater than 2005\n",
    "beyond_year_count = (df_age['ManufactureYear'] > 2005).sum()\n",
    "\n",
    "\n",
    "# Total count of null, empty, and zero values\n",
    "total_invalid_count = null_count + empty_count + same_year_count + beyond_year_count + zero_count \n",
    "\n",
    "print(f\"Number of null values in 'year': {null_count}\")\n",
    "print(f\"Number of empty values in 'year': {empty_count}\")\n",
    "print(f\"Number of zero values in 'year': {zero_count}\")\n",
    "\n",
    "print(f\"Number of same year in 'year': {same_year_count}\")\n",
    "print(f\"Number of beyond year in 'year': {beyond_year_count}\")\n",
    "print(f\"Total number of null, empty, or zero values in 'year': {total_invalid_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354d2adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting data if any invalid entries exists. Group by ManufactureYear and count occurrences\n",
    "year_counts = df_age['ManufactureYear'].value_counts().sort_index()\n",
    "\n",
    "# Plotting the bar chart\n",
    "plt.figure(figsize=(12, 6))  # Set the figure size\n",
    "year_counts.plot(kind='bar', color='skyblue', edgecolor='black')\n",
    "\n",
    "\n",
    "plt.title('Number of Aircraft by Manufacture Year')\n",
    "plt.xlabel('Manufacture Year')\n",
    "plt.ylabel('Number of Aircraft')\n",
    "\n",
    "\n",
    "for index, value in enumerate(year_counts):\n",
    "    plt.text(index, value, str(value), ha='center', va='bottom')\n",
    "\n",
    "# Show the plot\n",
    "plt.xticks(rotation=45)  \n",
    "plt.tight_layout()  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08be8320",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Calculating Aircraft Age & Delays\n",
    "# flight year is 2005 for all rows \n",
    "df_age['AircraftAge'] = 2005 - df_age['ManufactureYear']\n",
    "\n",
    "# Step 4: Group by AircraftAge and calculate average arrival delay\n",
    "avg_delay_by_age = df_age.groupby('AircraftAge')['ArrDelay'].mean().reset_index()\n",
    "\n",
    "Delay= avg_delay_by_age.sort_values(by = 'AircraftAge')\n",
    "Delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15d2a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing through line chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(avg_delay_by_age['AircraftAge'], avg_delay_by_age['ArrDelay'], linestyle='-')\n",
    "\n",
    "\n",
    "plt.title('Arrival Delay by Aircraft Age')\n",
    "plt.xlabel('Aircraft Age (Years)')\n",
    "plt.ylabel('Arrival Delay (Minutes)')\n",
    "\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c075f2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing through scatter plot\n",
    "\n",
    "# For example, let's collect all delays for each aircraft age\n",
    "delays_by_age = df_age.groupby('AircraftAge')['ArrDelay'].apply(list).reset_index()\n",
    "\n",
    "\n",
    "## Step 3: Plot the results\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Scatter plot for each age\n",
    "for index, row in delays_by_age.iterrows():\n",
    "    plt.scatter([row['AircraftAge']] * len(row['ArrDelay']), row['ArrDelay'], alpha=0.6)\n",
    "\n",
    "# Setting the title and labels\n",
    "plt.title('Arrival Delays Grouped by Aircraft Age')\n",
    "plt.xlabel('Aircraft Age (Years)')\n",
    "plt.ylabel('Arrival Delay (Minutes)')\n",
    "\n",
    "# Setting x-ticks to be integers\n",
    "plt.xticks(range(int(df_age['AircraftAge'].min()), int(df_age['AircraftAge'].max() + 1)))  \n",
    "\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373f991f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "delays_by_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c43640",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7dabd0ab",
   "metadata": {},
   "source": [
    "# 10-year period (1996 to 2005) Analysis\n",
    "\n",
    "### Evaluate whether older planes suffer more delays on a year-to-year basis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e521ed81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#INCLUDES \"DELAYS\" AND \"EARLY ARRIVALS\"\n",
    "\n",
    "# Average ABSOLUTE DELAYS.                                                                                                                                                                                  \n",
    "\n",
    "\n",
    "# List of years from 1996 to 2005 for the analysis\n",
    "years_of_analysis = range(1996, 2006)\n",
    "\n",
    "# Initialize an empty DataFrame to hold combined data for all years\n",
    "combined_df = pd.DataFrame()\n",
    "\n",
    "# Loop through each year\n",
    "for year_of_analysis in years_of_analysis:\n",
    "    # Step 1: Query only ArrDelay and TailNum from the current year and join with ManufactureYear\n",
    "    query = f'''\n",
    "    SELECT f.ArrDelay, f.TailNum, p.Year as ManufactureYear\n",
    "    FROM \"{year_of_analysis}\" f  \n",
    "    LEFT JOIN \"plane-data\" p ON f.TailNum = p.TailNum\n",
    "    WHERE f.Year = ?;  \n",
    "    '''\n",
    "    \n",
    "    # Execute the query with the year_of_analysis parameter\n",
    "    df = pd.read_sql_query(query, conn, params=(year_of_analysis,))\n",
    "\n",
    "    # Step 2: Data Preprocessing\n",
    "    df['ArrDelay'] = pd.to_numeric(df['ArrDelay'], errors='coerce')  # Ensure ArrDelay is numeric\n",
    "    df['ManufactureYear'] = pd.to_numeric(df['ManufactureYear'], errors='coerce')  # Convert ManufactureYear to numeric\n",
    "\n",
    "    df.dropna(subset=['ArrDelay', 'ManufactureYear'], inplace=True)  # Remove rows with missing\n",
    "\n",
    "    # Step 3: Count invalid values\n",
    "    same_year_count = (df['ManufactureYear'] == year_of_analysis).sum()\n",
    "    beyond_year_count = (df['ManufactureYear'] > year_of_analysis).sum()\n",
    "\n",
    "    # Identify indices of rows to drop\n",
    "    indices_to_drop = df[\n",
    "        (df['ManufactureYear'] == year_of_analysis) |  # Same year\n",
    "        (df['ManufactureYear'] > year_of_analysis) |   # Beyond year of analysis\n",
    "        (df['ManufactureYear'] == 0)                   # Zero values\n",
    "    ].index\n",
    "\n",
    "    # Drop those indices from the DataFrame\n",
    "    df.drop(indices_to_drop, inplace=True)\n",
    "\n",
    "    # Step 4: Calculate Aircraft Age\n",
    "    df['AircraftAge'] = year_of_analysis - df['ManufactureYear']\n",
    "\n",
    "    # Step 5: Take absolute value of Arrival Delay\n",
    "    df['AbsArrDelay'] = df['ArrDelay'].abs()\n",
    "\n",
    "    # Step 6: Group by AircraftAge and calculate average absolute arrival delay\n",
    "    avg_abs_delay_by_age = df.groupby('AircraftAge')['AbsArrDelay'].mean().reset_index()\n",
    "\n",
    "    # Add current year to the DataFrame\n",
    "    avg_abs_delay_by_age['YearOfAnalysis'] = year_of_analysis\n",
    "\n",
    "    # Append the current year's data to the combined DataFrame\n",
    "    combined_df = pd.concat([combined_df, avg_abs_delay_by_age], ignore_index=True)\n",
    "\n",
    "    # Step 7: Plot the line chart for each year\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(avg_abs_delay_by_age['AircraftAge'], avg_abs_delay_by_age['AbsArrDelay'], linestyle='-', marker='o', label=f'{year_of_analysis}')\n",
    "\n",
    "    # Step 8: Customize the plot for each year\n",
    "    plt.title(f'Average Absolute Arrival Delay by Aircraft Age - {year_of_analysis}')\n",
    "    plt.xlabel('Aircraft Age (Years)')\n",
    "    plt.ylabel('Average Absolute Arrival Delay (Minutes)')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    # Step 9: Plot scatter plot for the current year\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(df['AircraftAge'], df['AbsArrDelay'], alpha=0.5, c='blue')\n",
    "    plt.title(f'Scatter Plot: Aircraft Age vs Absolute Arrival Delay - {year_of_analysis}')\n",
    "    plt.xlabel('Aircraft Age (Years)')\n",
    "    plt.ylabel('Absolute Arrival Delay (Minutes)')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Show both plots\n",
    "    plt.show()\n",
    "\n",
    "    # Step 10: Calculate and print correlation between Aircraft Age and Arrival Delay\n",
    "    correlation = df['AircraftAge'].corr(df['AbsArrDelay'])\n",
    "    print(f\"Correlation between Aircraft Age and Absolute Arrival Delay for {year_of_analysis}: {correlation}\")\n",
    "\n",
    "    # Step 11: Create age groups for combined data\n",
    "    bins = [0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50]  # Define bins for age groups\n",
    "    labels = ['0-5', '6-10', '11-15', '16-20', '21-25', '26-30', '31-35', '36-40', '41-45', '45-50']\n",
    "    combined_df['AgeGroup'] = pd.cut(combined_df['AircraftAge'], bins=bins, labels=labels)\n",
    "\n",
    "    # Calculate average absolute delay by age group\n",
    "    avg_abs_delay_by_age_group = combined_df.groupby('AgeGroup')['AbsArrDelay'].mean().reset_index()\n",
    "    print(\"Average Absolute Arrival Delay by Aircraft Age Group:\")\n",
    "    print(avg_abs_delay_by_age_group)\n",
    "\n",
    "# Step 12: Plot the combined results\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Iterate over each year and plot the average absolute delay by aircraft age\n",
    "for year in years_of_analysis:\n",
    "    yearly_data = combined_df[combined_df['YearOfAnalysis'] == year]\n",
    "    plt.plot(yearly_data['AircraftAge'], yearly_data['AbsArrDelay'], label=f'Year {year}', marker='o', linestyle='-')\n",
    "\n",
    "# Step 13: Customize the plot\n",
    "plt.title('Average Absolute Arrival Delay by Aircraft Age (1996-2005)')\n",
    "plt.xlabel('Aircraft Age (Years)')\n",
    "plt.ylabel('Average Absolute Arrival Delay (Minutes)')\n",
    "plt.legend(title='Year')\n",
    "plt.grid(True)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics and year-on-year comparison\n",
    "summary_stats = combined_df.groupby('YearOfAnalysis').agg(\n",
    "    mean_delay=('AbsArrDelay', 'mean'),\n",
    "    median_delay=('AbsArrDelay', 'median'),\n",
    "    std_dev_delay=('AbsArrDelay', 'std'),\n",
    "    min_delay=('AbsArrDelay', 'min'),\n",
    "    max_delay=('AbsArrDelay', 'max')\n",
    ").reset_index()\n",
    "\n",
    "print(\"Summary Statistics for Each Year:\")\n",
    "print(summary_stats)\n",
    "\n",
    "# Calculate year-on-year change in mean delay\n",
    "summary_stats['YoY Change in Mean Delay'] = summary_stats['mean_delay'].diff()\n",
    "print(\"\\nYear-on-Year Change in Mean Absolute Delay:\")\n",
    "print(summary_stats[['YearOfAnalysis', 'YoY Change in Mean Delay']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f420dcaa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "yearly_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f28e77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a4bdf53d",
   "metadata": {},
   "source": [
    "### Removing Early Arrivals from the  Delays (Negative Values) \n",
    "### For Meaningful Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b70935",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#ONLY Delays (Not early Arrivals)\n",
    "\n",
    "# List of years from 1996 to 2005 for the analysis\n",
    "years_of_analysis = range(1996, 2006)\n",
    "\n",
    "# Empty DataFrame to hold combined data for all years\n",
    "combined_df = pd.DataFrame()\n",
    "\n",
    "# Looping through each year\n",
    "for year_of_analysis in years_of_analysis:\n",
    "    \n",
    "    query = f'''\n",
    "    SELECT f.ArrDelay, f.TailNum, p.Year as ManufactureYear\n",
    "    FROM \"{year_of_analysis}\" f  \n",
    "    LEFT JOIN \"plane-data\" p ON f.TailNum = p.TailNum\n",
    "    WHERE f.Year = ?;  \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    df = pd.read_sql_query(query, conn, params=(year_of_analysis,))\n",
    "\n",
    "    # Data Preprocessing\n",
    "    df['ArrDelay'] = pd.to_numeric(df['ArrDelay'], errors='coerce')  # Ensure ArrDelay is numeric\n",
    "    df['ManufactureYear'] = pd.to_numeric(df['ManufactureYear'], errors='coerce')  # Convert ManufactureYear to numeric\n",
    "\n",
    "    # Removing early arrivals (negative ArrDelay)\n",
    "    df = df[df['ArrDelay'] > 0]  \n",
    "\n",
    "    # Dropping rows with missing data \n",
    "    df.dropna(subset=['ArrDelay', 'ManufactureYear'], inplace=True)\n",
    "\n",
    "   \n",
    "    same_year_count = (df['ManufactureYear'] == year_of_analysis).sum()\n",
    "    beyond_year_count = (df['ManufactureYear'] > year_of_analysis).sum()\n",
    "\n",
    "   \n",
    "    indices_to_drop = df[\n",
    "        (df['ManufactureYear'] == year_of_analysis) |  # Same year\n",
    "        (df['ManufactureYear'] > year_of_analysis) |   # Beyond year of analysis\n",
    "        (df['ManufactureYear'] == 0)                   # Zero values\n",
    "    ].index\n",
    "\n",
    "    \n",
    "    df.drop(indices_to_drop, inplace=True)\n",
    "\n",
    "    # S Aircraft Age\n",
    "    df['AircraftAge'] = year_of_analysis - df['ManufactureYear']\n",
    "\n",
    "    # Taking absolute value of Arrival Delay (only positive delays remain)\n",
    "    df['LateArrDelay'] = df['ArrDelay'].abs()\n",
    "\n",
    "    # Group by AircraftAge and calculate average absolute arrival delay\n",
    "    avg_abs_delay_by_age = df.groupby('AircraftAge')['LateArrDelay'].mean().reset_index()\n",
    "\n",
    "    # Adding current year to the DataFrame\n",
    "    avg_abs_delay_by_age['YearOfAnalysis'] = year_of_analysis\n",
    "\n",
    "    # Appending the current year's data to the combined DataFrame\n",
    "    combined_df = pd.concat([combined_df, avg_abs_delay_by_age], ignore_index=True)\n",
    "\n",
    "    # Plotting the line chart for each year\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(avg_abs_delay_by_age['AircraftAge'], avg_abs_delay_by_age['LateArrDelay'], linestyle='-', marker='o', label=f'{year_of_analysis}')\n",
    "\n",
    "\n",
    "    plt.title(f'Average Late Arrival Delay by Aircraft Age - {year_of_analysis}')\n",
    "    plt.xlabel('Aircraft Age (Years)')\n",
    "    plt.ylabel('Average Late Arrival Delay (Minutes)')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    # Plotting scatter plot for the current year\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(df['AircraftAge'], df['LateArrDelay'], alpha=0.5, c='blue')\n",
    "    plt.title(f'Scatter Plot: Aircraft Age vs Late Arrival Delay - {year_of_analysis}')\n",
    "    plt.xlabel('Aircraft Age (Years)')\n",
    "    plt.ylabel('Late Arrival Delay (Minutes)')\n",
    "    plt.grid(True)\n",
    "    \n",
    "   \n",
    "    plt.show()\n",
    "\n",
    "    #  Statistical analysis between Aircraft Age and Absolute Arrival Delay\n",
    "    correlation = df['AircraftAge'].corr(df['LateArrDelay'])\n",
    "    print(f\"Correlation between Aircraft Age and LateArrDelay Arrival Delay for {year_of_analysis}: {correlation}\")\n",
    "\n",
    "    # Creating age groups for combined data\n",
    "    bins = [0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50]  # Define bins for age groups\n",
    "    labels = ['0-5', '6-10', '11-15', '16-20', '21-25', '26-30', '31-35', '36-40', '41-45', '45-50']\n",
    "    combined_df['AgeGroup'] = pd.cut(combined_df['AircraftAge'], bins=bins, labels=labels)\n",
    "\n",
    "    # Calculating average absolute delay by age group\n",
    "    avg_abs_delay_by_age_group = combined_df.groupby('AgeGroup')['LateArrDelay'].mean().reset_index()\n",
    "    print(\"Average Late Arrival Delay by Aircraft Age Group:\")\n",
    "    print(avg_abs_delay_by_age_group)\n",
    "\n",
    "# Plotting the combined results\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "\n",
    "for year in years_of_analysis:\n",
    "    yearly_data_delays = combined_df[combined_df['YearOfAnalysis'] == year]\n",
    "    plt.plot(yearly_data_delays['AircraftAge'], yearly_data_delays['LateArrDelay'], label=f'Year {year}', marker='o', linestyle='-')\n",
    "\n",
    "\n",
    "plt.title('Average Late Arrival Delay by Aircraft Age (1996-2005)')\n",
    "plt.xlabel('Aircraft Age (Years)')\n",
    "plt.ylabel('Average Late Arrival Delay (Minutes)')\n",
    "plt.legend(title='Year')\n",
    "plt.grid(True)\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics and Y/Y comparison\n",
    "summary_stats = combined_df.groupby('YearOfAnalysis').agg(\n",
    "    mean_delay=('LateArrDelay', 'mean'),\n",
    "    median_delay=('LateArrDelay', 'median'),\n",
    "    std_dev_delay=('LateArrDelay', 'std'),\n",
    "    min_delay=('LateArrDelay', 'min'),\n",
    "    max_delay=('LateArrDelay', 'max')\n",
    ").reset_index()\n",
    "\n",
    "print(\"Summary Statistics for Each Year:\")\n",
    "print(summary_stats)\n",
    "\n",
    "# Y/Y change in mean delay\n",
    "summary_stats['YoY Change in Mean Delay'] = summary_stats['mean_delay'].diff()\n",
    "print(\"\\nYear-on-Year Change in Mean Absolute Delay:\")\n",
    "print(summary_stats[['YearOfAnalysis', 'YoY Change in Mean Delay']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37b0ad3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "summary_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e04851",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "yearly_data_delays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db231e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "yearly_data_delays.to_csv('yearly_data_delays.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee79741a",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f8ab42",
   "metadata": {},
   "source": [
    "# Answer to Question 2 c)  \n",
    "\n",
    "### For each year, fit a logistic regression model for the probability of diverted US flights using as many features as possible from attributes of the departure date, the sched- uled departure and arrival times, the coordinates and distance between departure and planned arrival airports, and the carrier. Visualize the coefficients across years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2254ac8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    f1_score, \n",
    "    roc_curve, \n",
    "    roc_auc_score, \n",
    "    precision_recall_curve, \n",
    "    classification_report, \n",
    "    confusion_matrix\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0e4a63",
   "metadata": {},
   "source": [
    "### Analyzing one year first (1998)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f859998",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Querying the relevant features from the 1998 table and joining with the carriers table\n",
    "query = '''\n",
    "SELECT \n",
    "    f.Month, \n",
    "    f.DayofMonth,  \n",
    "    f.CRSArrTime AS ScheduledArrival, \n",
    "    f.CRSDepTime AS ScheduledDeparture, \n",
    "    f.Distance,       \n",
    "    f.Diverted, \n",
    "    c.Description AS CarrierName,\n",
    "    a1.lat AS Origin_Latitude,\n",
    "    a1.long AS Origin_Longitude,\n",
    "    a2.lat AS Dest_Latitude,\n",
    "    a2.long AS Dest_Longitude\n",
    "FROM \n",
    "    \"1998\" f \n",
    "LEFT JOIN \n",
    "    \"carriers\" c ON f.UniqueCarrier = c.Code\n",
    "LEFT JOIN \n",
    "    \"airports\" a1 ON f.Origin = a1.iata \n",
    "LEFT JOIN \n",
    "    \"airports\" a2 ON f.Dest = a2.iata   \n",
    "WHERE \n",
    "    f.Diverted IS NOT NULL;\n",
    "'''\n",
    "\n",
    "\n",
    "df_1998 = pd.read_sql_query(query, conn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d843f213",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1998.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e7b8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1998.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4c2ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1998.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec3e3f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Converting categorical variables to one-hot encoding\n",
    "df_encoded = pd.get_dummies(df_1998, columns=['CarrierName', 'Month', 'DayofMonth'], drop_first=True)\n",
    "\n",
    "# Defining X and Y\n",
    "X = df_encoded.drop(columns=['Diverted'])\n",
    "y = df_encoded['Diverted']\n",
    "\n",
    "# Splittiing the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Fitting the logistic regression model\n",
    "model = LogisticRegression(max_iter=1000, class_weight = 'balanced')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Model evaluation\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculating accuracy, generate confusion matrix and classification report\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "\n",
    "# Model coefficients\n",
    "coefficients = model.coef_[0]\n",
    "features = X.columns\n",
    "for feature, coeff in zip(features, coefficients):\n",
    "    print(f\"Feature: {feature}, Coefficient: {coeff:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd32eb57",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Creating a DataFrame for coefficients and features\n",
    "coef_df = pd.DataFrame({\n",
    "    'Feature': features,\n",
    "    'Coefficient': coefficients\n",
    "})\n",
    "\n",
    "\n",
    "# Absolute values for sorting\n",
    "coef_df['Abs_Coefficient'] = np.abs(coef_df['Coefficient'])\n",
    "\n",
    "# Absolute coefficient in descending order\n",
    "sorted_coef_df = coef_df.sort_values(by='Abs_Coefficient', ascending=False)\n",
    "\n",
    "# Top 20 highest coefficients\n",
    "print(\"\\nTop 20 Features by Absolute Coefficient:\")\n",
    "sorted_coef_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bf8be8",
   "metadata": {},
   "source": [
    "## The model performs poorly in predicting diverted flights due to class imbalance. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ade18d",
   "metadata": {},
   "source": [
    "# Trying with Reduced features for year 1998 \n",
    "\n",
    "### based on selecting features from top 20 coefficients "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bb79ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query 2 with less features\n",
    "\n",
    "# Querying the relevant features from the 1998 table and joining with the carriers table\n",
    "query_2 = '''\n",
    "SELECT \n",
    "    f.Month, \n",
    "    f.DayofMonth,        \n",
    "    f.Diverted, \n",
    "    c.Description AS CarrierName,\n",
    "    a1.lat AS Origin_Latitude,\n",
    "    a1.long AS Origin_Longitude,\n",
    "    a2.lat AS Dest_Latitude,\n",
    "    a2.long AS Dest_Longitude\n",
    "FROM \n",
    "    \"1998\" f \n",
    "LEFT JOIN \n",
    "    \"carriers\" c ON f.UniqueCarrier = c.Code\n",
    "LEFT JOIN \n",
    "    \"airports\" a1 ON f.Origin = a1.iata \n",
    "LEFT JOIN \n",
    "    \"airports\" a2 ON f.Dest = a2.iata   \n",
    "WHERE \n",
    "    f.Diverted IS NOT NULL;\n",
    "'''\n",
    "\n",
    "\n",
    "df_1998 = pd.read_sql_query(query_2, conn)\n",
    "\n",
    "df_1998.dropna(inplace = True)\n",
    "\n",
    "# Counting the number of diverted and non-diverted flights\n",
    "diverted_counts = df_1998['Diverted'].value_counts()\n",
    "\n",
    "\n",
    "print(\"Number of non-diverted flights (0):\", diverted_counts.get(0, 0))\n",
    "print(\"Number of diverted flights (1):\", diverted_counts.get(1, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9873115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting categorical variables to one-hot encoding\n",
    "df_encoded = pd.get_dummies(df_1998, columns=['CarrierName', 'Month', 'DayofMonth'], drop_first=True)\n",
    "\n",
    "# Defining X and Y\n",
    "X = df_encoded.drop('Diverted', axis=1)\n",
    "y = df_encoded['Diverted']\n",
    "\n",
    "# Splittiing the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Fitting the logistic regression model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Calculating coefficients and odds ratios\n",
    "coefficients = model.coef_[0]\n",
    "features = X.columns\n",
    "odds_ratios = np.exp(coefficients)\n",
    "\n",
    "\n",
    "coef_df = pd.DataFrame({\n",
    "    'Feature': features,\n",
    "    'Coefficient': coefficients,\n",
    "    'Odds_Ratio': odds_ratios\n",
    "})\n",
    "\n",
    "# Absolute values for sorting\n",
    "coef_df['Abs_Coefficient'] = np.abs(coefficients)\n",
    "\n",
    "# Highest and lowest impact variables\n",
    "highest_impact = coef_df.sort_values(by='Abs_Coefficient', ascending=False).head(1)\n",
    "lowest_impact = coef_df.sort_values(by='Abs_Coefficient', ascending=True).head(1)\n",
    "\n",
    "\n",
    "print(\"Variable with Highest Impact:\")\n",
    "print(highest_impact[['Feature', 'Coefficient', 'Odds_Ratio']])\n",
    "\n",
    "print(\"\\nVariable with Least Impact:\")\n",
    "print(lowest_impact[['Feature', 'Coefficient', 'Odds_Ratio']])\n",
    "\n",
    "# Model Evaluation\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(f\"\\nAccuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594847cd",
   "metadata": {},
   "source": [
    "## The model performs poorly again in predicting diverted flights due to class imbalance. \n",
    "<br> Sampling (Under or Over) should be used. Also for such datasets, Logistics Regression might not be the best model (Other models are out of scope of this Assignment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa6de8f",
   "metadata": {},
   "source": [
    "## Need to Perform Undersampling to balance the data for Diversions and non-Diversion\n",
    "<br>\n",
    "As reducing features is not adding meaning to the analysis due to highly skewed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc1dbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Querying the relevant features (similar to previous query)\n",
    "query_undersample = '''\n",
    "SELECT \n",
    "    f.Month, \n",
    "    f.DayofMonth,        \n",
    "    f.Diverted, \n",
    "    c.Description AS CarrierName,\n",
    "    a1.lat AS Origin_Latitude,\n",
    "    a1.long AS Origin_Longitude,\n",
    "    a2.lat AS Dest_Latitude,\n",
    "    a2.long AS Dest_Longitude\n",
    "FROM \n",
    "    \"1998\" f \n",
    "LEFT JOIN \n",
    "    \"carriers\" c ON f.UniqueCarrier = c.Code\n",
    "LEFT JOIN \n",
    "    \"airports\" a1 ON f.Origin = a1.iata \n",
    "LEFT JOIN \n",
    "    \"airports\" a2 ON f.Dest = a2.iata   \n",
    "WHERE \n",
    "    f.Diverted IS NOT NULL;\n",
    "'''\n",
    "\n",
    "\n",
    "df_1998 = pd.read_sql_query(query_undersample, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1d4fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_encoded = pd.get_dummies(df_1998, drop_first=True)\n",
    "\n",
    "X = df_encoded.drop('Diverted', axis=1)\n",
    "y = df_encoded['Diverted']\n",
    "\n",
    "# Applying Random Undersampling to balance the classes\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_resampled, y_resampled = rus.fit_resample(X, y)\n",
    "\n",
    "# New class distribution\n",
    "print(\"Class distribution after undersampling:\", y_resampled.value_counts())\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "coefficients = model.coef_[0]\n",
    "features = X.columns\n",
    "odds_ratios = np.exp(coefficients)\n",
    "\n",
    "\n",
    "coef_df = pd.DataFrame({\n",
    "    'Feature': features,\n",
    "    'Coefficient': coefficients,\n",
    "    'Odds_Ratio': odds_ratios\n",
    "})\n",
    "\n",
    "\n",
    "coef_df['Abs_Coefficient'] = np.abs(coefficients)\n",
    "\n",
    "\n",
    "highest_impact = coef_df.sort_values(by='Abs_Coefficient', ascending=False).head(1)\n",
    "lowest_impact = coef_df.sort_values(by='Abs_Coefficient', ascending=True).head(1)\n",
    "\n",
    "\n",
    "print(\"Variable with Highest Impact:\")\n",
    "print(highest_impact[['Feature', 'Coefficient', 'Odds_Ratio']])\n",
    "\n",
    "print(\"\\nVariable with Least Impact:\")\n",
    "print(lowest_impact[['Feature', 'Coefficient', 'Odds_Ratio']])\n",
    "\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(f\"\\nAccuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a7d519",
   "metadata": {},
   "source": [
    "### With undersampling, some meaning to analysis is achieved, however huge dataset is lost and is not the true reflection of the features impacting diversion. \n",
    "\n",
    "### Logistics regression might not be the right model for predicting diversions. However, other modeling techniques are out of scope for this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4c3d7e",
   "metadata": {},
   "source": [
    "---\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75f9885",
   "metadata": {},
   "source": [
    "# Analyzing 10-years period (1996-2005)   \n",
    "\n",
    "### Seems Undersampling is better way to go with the diversion analysis as data is heavily skewed towards non-diversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe987f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Range of years for the analysis\n",
    "years_of_analysis = list(range(1996, 2006))\n",
    "\n",
    "# List to hold data for all years\n",
    "all_years_data = []\n",
    "\n",
    "# Querying Data in a Loop\n",
    "for year in years_of_analysis:\n",
    "    print(f\"Querying data for the year: {year}\")\n",
    "\n",
    "    # Querying the data for the specific year\n",
    "    query = f'''\n",
    "    SELECT \n",
    "        f.Month, \n",
    "        f.DayofMonth,        \n",
    "        f.Diverted, \n",
    "        f.CRSArrTime AS ScheduledArrival, \n",
    "        f.CRSDepTime AS ScheduledDeparture, \n",
    "        f.Distance,\n",
    "        c.Description AS CarrierName,\n",
    "        a1.lat AS Origin_Latitude,\n",
    "        a1.long AS Origin_Longitude,\n",
    "        a2.lat AS Dest_Latitude,\n",
    "        a2.long AS Dest_Longitude\n",
    "    FROM \n",
    "        \"{year}\" f \n",
    "    LEFT JOIN \n",
    "        \"carriers\" c ON f.UniqueCarrier = c.Code\n",
    "    LEFT JOIN \n",
    "        \"airports\" a1 ON f.Origin = a1.iata \n",
    "    LEFT JOIN \n",
    "        \"airports\" a2 ON f.Dest = a2.iata   \n",
    "    WHERE \n",
    "        f.Diverted IS NOT NULL;\n",
    "    '''\n",
    "\n",
    "    \n",
    "    df_year = pd.read_sql_query(query, conn)\n",
    "    df_year['Year'] = year \n",
    "    all_years_data.append(df_year)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6ce2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining all the yearly data into a single DataFrame\n",
    "combined_data = pd.concat(all_years_data, ignore_index=True)\n",
    "\n",
    "# Dropping missing values \n",
    "combined_data.dropna(inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b680e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding for categorical variables\n",
    "df_encoded_all = pd.get_dummies(combined_data, columns=['CarrierName', 'Month'], drop_first=True)\n",
    "\n",
    "# Defining X and Y for logistic regression\n",
    "X = df_encoded_all.drop('Diverted', axis=1)\n",
    "y = df_encoded_all['Diverted']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b7ff5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PData under-sampling\n",
    "\n",
    "print(\"Data before undersampling:\")\n",
    "\n",
    "# Distribution of target variable before undersampling\n",
    "print(\"Counts of data before undersampling:\")\n",
    "print(y_train.value_counts())\n",
    "\n",
    "# Shape of the feature set before undersampling\n",
    "print(f\"Shape of X_train before undersampling: {X_train.shape}\")\n",
    "print(f\"Shape of y_train before undersampling: {y_train.shape}\")\n",
    "\n",
    "# Random Under-sampling to balance the data\n",
    "undersampler = RandomUnderSampler(random_state=42)\n",
    "X_resampled, y_resampled = undersampler.fit_resample(X_train, y_train)\n",
    "\n",
    "# Counts of the resampled data to check if the classes are balanced\n",
    "print(\"\\nCounts of resampled data after undersampling:\")\n",
    "print(y_resampled.value_counts())\n",
    "\n",
    "# Shape of the resampled feature set\n",
    "print(f\"Shape of X_train after undersampling: {X_resampled.shape}\")\n",
    "print(f\"Shape of y_train after undersampling: {y_resampled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ea6820",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Logistic regression model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_resampled, y_resampled)\n",
    "\n",
    "# Predictions and probabilities\n",
    "y_pred = model.predict(X_test)\n",
    "y_prob = model.predict_proba(X_test)[:, 1]  # Get the predicted probabilities for class 1\n",
    "\n",
    "# Coefficients for the combined data\n",
    "coefficients = model.coef_[0]\n",
    "features = X.columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ec3e60",
   "metadata": {},
   "source": [
    "#### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0b53bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Coefficient Table\n",
    "coef_df = pd.DataFrame({\n",
    "    \"Feature\": features,\n",
    "    \"Coefficient\": coefficients\n",
    "}).sort_values(by=\"Coefficient\", ascending=False)\n",
    "\n",
    "print(\"\\nFeature Coefficients:\")\n",
    "print(coef_df)\n",
    "\n",
    "# Accuracy, Precision, Recall, F1-Score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"\\nModel Evaluation Metrics:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Classification Report\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(report)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ea59db",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_df.to_csv('coef_df.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b19460",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb3ee99",
   "metadata": {},
   "outputs": [],
   "source": [
    "report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6854ff80",
   "metadata": {},
   "source": [
    "#### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df97c321",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ROC Curve and AUC Score\n",
    "fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "roc_auc = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='grey')\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Bar Plot for Coefficients\n",
    "plt.figure(figsize=(10, 6))\n",
    "coef_df.plot(kind='barh', x='Feature', y='Coefficient', legend=False)\n",
    "plt.title(\"Logistic Regression Coefficients\")\n",
    "plt.xlabel(\"Coefficient Value\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Histogram of Predicted Probabilities\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(y_prob, bins=20, color='skyblue', edgecolor='black')\n",
    "plt.title(\"Histogram of Predicted Probabilities\")\n",
    "plt.xlabel(\"Predicted Probability of Diversion\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5911ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
